{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB GenAI - LLMs - OpenAI GPT API Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Read the API key from the file\n",
    "with open(\"../../API_Key.txt\", \"r\") as file:\n",
    "    open_api_key = file.read().strip()\n",
    "    \n",
    "print(f\"API Key: '{open_api_key}'\")  # Print to confirm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Conversation\n",
    "**Exercise:** Create a simple chatbot that can answer basic questions about a given topic (e.g., history, technology).  \n",
    "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `n`, `stop`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hello! How can I assist you today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Initialize OpenAI client with your API key\n",
    "client = openai.OpenAI(api_key=open_api_key)\n",
    "\n",
    "def chatbot_response(user_input, model=\"gpt-3.5-turbo\", temperature=0.7, max_tokens=150, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, stop=None):\n",
    "    \"\"\"\n",
    "    Generates a response from the chatbot based on the user's input.\n",
    "    \n",
    "    Parameters:\n",
    "        user_input (str): The user's question or message.\n",
    "        model (str): The OpenAI model to use.\n",
    "        temperature (float): Controls randomness (0 = deterministic, 1 = very random).\n",
    "        max_tokens (int): The maximum number of tokens to generate.\n",
    "        top_p (float): Controls diversity via nucleus sampling.\n",
    "        frequency_penalty (float): Lowers repetition of high-frequency words.\n",
    "        presence_penalty (float): Encourages more diverse topics.\n",
    "        n (int): Number of responses to generate.\n",
    "        stop (str or list): Stop sequences to prevent further response generation.\n",
    "    \n",
    "    Returns:\n",
    "        str: The chatbot's response.\n",
    "    \"\"\"\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a knowledgeable chatbot that answers questions clearly and concisely.\"},\n",
    "                {\"role\": \"user\", \"content\": user_input}]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        presence_penalty=presence_penalty,\n",
    "        n=n,\n",
    "        stop=stop\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example Usage\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "        print(\"Chatbot: Goodbye!\")\n",
    "        break\n",
    "    response = chatbot_response(user_input)\n",
    "    print(f\"Chatbot: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Summarization\n",
    "**Exercise:** Write a script that takes a long text input and summarizes it into a few sentences.  \n",
    "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `best_of`, `logprobs`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: The Industrial Revolution was a transformative era of industrialization and innovation from the late 18th to the 19th century, marked by the shift from manual labor to machine-based manufacturing. It brought about technological advancements, economic growth, and urbanization, but also led to environmental pollution and harsh working conditions for laborers.\n"
     ]
    }
   ],
   "source": [
    "def summarize_text(text, model=\"gpt-3.5-turbo\", temperature=0.5, max_tokens=100, top_p=1, frequency_penalty=0, presence_penalty=0, logprobs=None):\n",
    "    \"\"\"\n",
    "    Summarizes a long text into a concise version.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input text to summarize.\n",
    "        model (str): The OpenAI model to use.\n",
    "        temperature (float): Controls randomness (0 = deterministic, 1 = very random).\n",
    "        max_tokens (int): The maximum number of tokens for the summary.\n",
    "        top_p (float): Controls diversity via nucleus sampling.\n",
    "        frequency_penalty (float): Reduces repetition of high-frequency words.\n",
    "        presence_penalty (float): Encourages introducing new words and topics.\n",
    "        best_of (int): Generates multiple completions and returns the best one.\n",
    "        logprobs (int or None): Returns log probabilities of tokens if set.\n",
    "\n",
    "    Returns:\n",
    "        str: The summarized text.\n",
    "    \"\"\"\n",
    "    prompt = f\"Summarize the following text in a few sentences:\\n\\n{text}\\n\\nSummary:\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are an advanced summarization assistant.\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        presence_penalty=presence_penalty,\n",
    "        n=1,\n",
    "        # best_of=best_of, Not supported in newer versions of openai\n",
    "        logprobs=logprobs\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example Usage\n",
    "long_text = \"\"\"The Industrial Revolution was a period of major industrialization and innovation that began in the late 18th century and continued into the 19th century. \n",
    "It saw the transition from manual labor and hand-made goods to machine-based manufacturing and mass production. This revolution brought significant advancements in \n",
    "technology, transportation, and economic growth, leading to urbanization and changes in social structures. However, it also caused environmental pollution and poor \n",
    "working conditions for laborers.\"\"\"\n",
    "\n",
    "summary = summarize_text(long_text)\n",
    "print(\"Summary:\", summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Translation\n",
    "**Exercise:** Develop a tool that translates text from one language to another using the API.  \n",
    "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `echo`, `logit_bias`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated Text: Bonjour, comment vas-tu ?\n"
     ]
    }
   ],
   "source": [
    "def translate_text(\n",
    "    text, \n",
    "    source_lang=\"English\", \n",
    "    target_lang=\"Spanish\", \n",
    "    model=\"gpt-3.5-turbo\", \n",
    "    temperature=0.3, \n",
    "    max_tokens=200, \n",
    "    top_p=1, \n",
    "    frequency_penalty=0, \n",
    "    presence_penalty=0, \n",
    "    echo=False, \n",
    "    logit_bias=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Translates text from one language to another.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The text to translate.\n",
    "        source_lang (str): The original language of the text.\n",
    "        target_lang (str): The target language for translation.\n",
    "        model (str): The OpenAI model to use.\n",
    "        temperature (float): Controls randomness (0 = deterministic, 1 = very creative).\n",
    "        max_tokens (int): The maximum number of tokens for the output.\n",
    "        top_p (float): Controls diversity via nucleus sampling.\n",
    "        frequency_penalty (float): Reduces repetition of high-frequency words.\n",
    "        presence_penalty (float): Encourages introducing new words and topics.\n",
    "        echo (bool): If True, repeats the input in the response.\n",
    "        logit_bias (dict or None): Biases the probability of specific tokens.\n",
    "\n",
    "    Returns:\n",
    "        str: The translated text.\n",
    "    \"\"\"\n",
    "    prompt = f\"Translate the following text from {source_lang} to {target_lang}:\\n\\n{text}\\n\\nTranslation:\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are an expert translator.\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        presence_penalty=presence_penalty\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# Example Usage\n",
    "text_to_translate = \"Hello, how are you?\"\n",
    "translated_text = translate_text(text_to_translate, source_lang=\"English\", target_lang=\"French\")\n",
    "print(\"Translated Text:\", translated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sentiment Analysis\n",
    "**Exercise:** Implement a sentiment analysis tool that determines the sentiment of a given text (positive, negative, neutral).  \n",
    "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `n`, `logprobs`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "def analyze_sentiment(text, model=\"gpt-3.5-turbo\", temperature=0.0, max_tokens=10, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, logprobs=None):\n",
    "    \"\"\"\n",
    "    Determines the sentiment (Positive, Negative, Neutral) of the given text.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input text for sentiment analysis.\n",
    "        model (str): The OpenAI model to use.\n",
    "        temperature (float): Controls randomness (0 = deterministic, 1 = very creative).\n",
    "        max_tokens (int): The maximum number of tokens for the response.\n",
    "        top_p (float): Controls diversity via nucleus sampling.\n",
    "        frequency_penalty (float): Reduces repetition of high-frequency words.\n",
    "        presence_penalty (float): Encourages introducing new words and topics.\n",
    "        n (int): Number of responses to generate.\n",
    "        logprobs (int or None): Returns log probabilities if set.\n",
    "\n",
    "    Returns:\n",
    "        str: The detected sentiment (Positive, Negative, Neutral).\n",
    "    \"\"\"\n",
    "    prompt = f\"Analyze the sentiment of the following text and respond with only 'Positive', 'Negative', or 'Neutral':\\n\\n{text}\\n\\nSentiment:\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a sentiment analysis expert.\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        presence_penalty=presence_penalty,\n",
    "        n=n\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# Example Usage\n",
    "text_to_analyze = \"I love this product! It's amazing and exceeded my expectations.\"\n",
    "sentiment = analyze_sentiment(text_to_analyze)\n",
    "print(\"Sentiment:\", sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Completion\n",
    "**Exercise:** Create a text completion application that generates text based on an initial prompt.  \n",
    "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `stop`, `best_of`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion 1: there lived a wise and benevolent queen named Seraphina. Queen Seraphina was loved by all her subjects for her kindness, wisdom, and fair rule. The kingdom flourished under her reign, and the people lived in peace and harmony. However, one day, a dark shadow fell upon the land as a powerful sorcerer named Malakar emerged from the depths of the forest.\n",
      "\n",
      "Completion 2: there was a magnificent castle perched atop a hill, overlooking lush green valleys and sparkling rivers. The castle was home to a wise and just king, known for his fairness and generosity towards his people. The kingdom prospered under his rule, with peace and harmony prevailing throughout the land.\n",
      "\n",
      "Completion 3: there lived a brave knight named Sir Edward. Sir Edward was known throughout the land for his valor and unwavering loyalty to the king. One day, a terrible dragon began terrorizing the kingdom, burning villages and kidnapping villagers. The king called upon Sir Edward to slay the dragon and save the kingdom from destruction. Sir Edward accepted the challenge without hesitation, knowing the dangers that lay ahead. Armed with his trusty sword and his unwavering courage, Sir Edward set out on a perilous\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_text(prompt, model=\"gpt-3.5-turbo\", temperature=0.7, max_tokens=100, top_p=1, frequency_penalty=0, presence_penalty=0, stop=None, n=1):\n",
    "    \"\"\"\n",
    "    Generates text based on an initial prompt.\n",
    "\n",
    "    Parameters:\n",
    "        prompt (str): The input text to continue.\n",
    "        model (str): The OpenAI model to use.\n",
    "        temperature (float): Controls randomness (0 = deterministic, 1 = very creative).\n",
    "        max_tokens (int): The maximum number of tokens for the response.\n",
    "        top_p (float): Controls diversity via nucleus sampling.\n",
    "        frequency_penalty (float): Reduces repetition of high-frequency words.\n",
    "        presence_penalty (float): Encourages introducing new words and topics.\n",
    "        stop (str or list): Optional stopping sequences to control output.\n",
    "        n (int): Number of completions to generate.\n",
    "\n",
    "    Returns:\n",
    "        list: Generated text completions.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a creative AI assistant.\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        presence_penalty=presence_penalty,\n",
    "        stop=stop,\n",
    "        n=n\n",
    "    )\n",
    "    \n",
    "    completions = [choice.message.content.strip() for choice in response.choices]\n",
    "    return completions\n",
    "\n",
    "# Example Usage\n",
    "prompt_text = \"Once upon a time in a distant kingdom,\"\n",
    "generated_texts = generate_text(prompt_text, n=3)\n",
    "\n",
    "for i, text in enumerate(generated_texts, 1):\n",
    "    print(f\"Completion {i}: {text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS: Google Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Conversation\n",
    "**Exercise:** Create a basic chatbot using Google Vertex AI to answer questions about a given topic.  \n",
    "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `n`, `stop`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Summarization\n",
    "**Exercise:** Develop a script that summarizes long text inputs using Google Vertex AI.  \n",
    "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `best_of`, `logprobs`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Translation\n",
    "**Exercise:** Create a tool that translates text from one language to another using Google Vertex AI.  \n",
    "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `echo`, `logit_bias`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sentiment Analysis\n",
    "**Exercise:** Implement a sentiment analysis tool using Google Vertex AI to determine the sentiment of a given text.  \n",
    "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `n`, `logprobs`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Completion\n",
    "**Exercise:** Develop a text completion application using Google Vertex AI to generate text based on an initial prompt.  \n",
    "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `stop`, `best_of`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
